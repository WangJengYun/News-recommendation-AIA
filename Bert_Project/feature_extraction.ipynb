{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.extract_features import InputExample\n",
    "from bert import modeling, tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jovyan/at081-group39/python_code/Bert_Project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation_for_feature_extraction import InputExample, loading_pickle_file, Inputdata_for_FE, convert_examples_to_features, input_fn_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = ['-1,-2,-3,-4'] \n",
    "extract_layers = layers\n",
    "bert_config_file = 'chinese_L-12_H-768_A-12/bert_config.json'\n",
    "vocab_file = 'chinese_L-12_H-768_A-12/vocab.txt'\n",
    "init_checkpoint = 'chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "num_tpu_cores = 8\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 0\n",
      "INFO:tensorflow:tokens: [CLS] 三 大 電 腦 物 流 業 者 去 年 倒 帳 損 失 超 過 1 億 [SEP] 1998 年 由 於 景 氣 低 迷 , 連 帶 倒 帳 風 險 也 大 為 提 高 , 國 內 三 大 物 流 商 聯 強 、 捷 元 、 展 碁 去 年 倒 帳 金 額 , 合 計 已 經 逾 新 台 幣 1 億 元 , 由 於 物 流 業 毛 利 率 在 5 % 以 下 , 淨 利 率 更 低 於 1 % , 風 險 損 失 金 額 逼 近 此 一 比 例 的 業 者 , 將 難 以 立 足 市 場 。 [SEP]\n",
      "INFO:tensorflow:input_ids: 101 676 1920 7442 5582 4289 3837 3511 5442 1343 2399 948 2379 3010 1927 6631 6882 122 1023 102 8368 2399 4507 3176 3250 3706 856 6837 117 6865 2380 948 2379 7591 7402 738 1920 4158 2990 7770 117 1751 1058 676 1920 4289 3837 1555 5474 2485 510 2949 1039 510 2245 4805 1343 2399 948 2379 7032 7540 117 1394 6243 2347 5195 6874 3173 1378 2395 122 1023 1039 117 4507 3176 4289 3837 3511 3688 1164 4372 1762 126 110 809 678 117 3912 1164 4372 3291 856 3176 122 110 117 7591 7402 3010 1927 7032 7540 6873 6818 3634 671 3683 891 4638 3511 5442 117 2200 7432 809 4989 6639 2356 1842 511 102 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 1\n",
      "INFO:tensorflow:tokens: [CLS] 晶 圓 代 工 市 場 發 燒 台 積 電 買 德 碁 多 一 座 八 吋 廠 [SEP] 台 積 電 將 出 資 金 額 應 在 新 台 幣 80 億 元 以 上 ， 在 近 期 內 宣 佈 購 入 德 碁 半 導 體 公 司 三 成 股 權 ， 德 碁 也 將 成 為 台 積 電 另 一 個 晶 圓 代 工 的 場 所 。 股 權 改 組 後 的 德 碁 ， 原 本 最 大 單 一 法 人 股 東 宏 電 ， 持 股 將 由 目 前 的 49 . 3 % 降 至 30 % ， 而 德 碁 也 將 於 88 年 [SEP]\n",
      "INFO:tensorflow:input_ids: 101 3253 1755 807 2339 2356 1842 4634 4240 1378 4948 7442 6525 2548 4805 1914 671 2429 1061 1397 2449 102 1378 4948 7442 2200 1139 6536 7032 7540 2746 1762 3173 1378 2395 8188 1023 1039 809 677 8024 1762 6818 3309 1058 2146 854 6554 1057 2548 4805 1288 2206 7768 1062 1385 676 2768 5500 3609 8024 2548 4805 738 2200 2768 4158 1378 4948 7442 1369 671 943 3253 1755 807 2339 4638 1842 2792 511 5500 3609 3121 5175 2527 4638 2548 4805 8024 1333 3315 3297 1920 1606 671 3791 782 5500 3346 2131 7442 8024 2898 5500 2200 4507 4680 1184 4638 8249 119 124 110 7360 5635 8114 110 8024 5445 2548 4805 738 2200 3176 8302 2399 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 2\n",
      "INFO:tensorflow:tokens: [CLS] 味 王 樹 林 廠 申 請 變 更 被 退 回 [SEP] 味 王 食 品 樹 林 廠 申 請 變 更 住 商 混 合 區 案 ， 8 日 遭 台 北 縣 都 市 計 畫 委 員 會 正 式 發 文 退 回 ， 並 要 求 味 王 重 新 提 出 整 體 開 發 規 劃 、 補 足 相 關 文 件 後 再 審 。 味 王 台 北 樹 林 廠 面 積 共 約 1 萬 8 , 000 多 坪 ， 從 88 年 初 起 即 陸 續 資 遣 員 工 ， 並 將 生 產 線 遷 往 雲 林 豐 田 廠 。 [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1456 4374 3572 3360 2449 4509 6313 6365 3291 6158 6842 1726 102 1456 4374 7608 1501 3572 3360 2449 4509 6313 6365 3291 857 1555 3921 1394 1281 3428 8024 129 3189 6901 1378 1266 5238 6963 2356 6243 4529 1999 1519 3298 3633 2466 4634 3152 6842 1726 8024 699 6206 3724 1456 4374 7028 3173 2990 1139 3146 7768 7274 4634 6211 1205 510 6171 6639 4685 7302 3152 816 2527 1086 2182 511 1456 4374 1378 1266 3572 3360 2449 7481 4948 1066 5147 122 5857 129 117 8241 1914 1790 8024 2537 8302 2399 1159 6629 1315 7380 5265 6536 6897 1519 2339 8024 699 2200 4495 4496 5221 6907 2518 7437 3360 6493 4506 2449 511 102 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 3\n",
      "INFO:tensorflow:tokens: [CLS] 多 家 印 刷 電 路 板 上 市 公 司 相 繼 接 獲 國 際 大 廠 訂 單 [SEP] 印 刷 電 路 板 多 家 印 刷 電 路 板 上 市 公 司 ， 相 繼 接 獲 國 際 大 廠 訂 單 ， 預 估 6 月 營 業 收 入 將 優 於 5 月 ， 營 運 漸 入 佳 境 ， 88 年 5 月 營 收 比 87 年 同 期 成 長 2 成 以 上 的 印 刷 電 路 板 上 市 公 司 ， 有 金 像 電 ， 燿 華 ， 敬 鵬 等 3 家 ， 營 收 衰 退 的 公 司 ， 有 清 三 ， 華 [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1914 2157 1313 1170 7442 6662 3352 677 2356 1062 1385 4685 5262 2970 4363 1751 7396 1920 2449 6242 1606 102 1313 1170 7442 6662 3352 1914 2157 1313 1170 7442 6662 3352 677 2356 1062 1385 8024 4685 5262 2970 4363 1751 7396 1920 2449 6242 1606 8024 7521 844 127 3299 4245 3511 3119 1057 2200 1032 3176 126 3299 8024 4245 6880 4041 1057 881 1862 8024 8302 2399 126 3299 4245 3119 3683 8467 2399 1398 3309 2768 7269 123 2768 809 677 4638 1313 1170 7442 6662 3352 677 2356 1062 1385 8024 3300 7032 1008 7442 8024 4254 5836 8024 3143 7868 5023 124 2157 8024 4245 3119 6139 6842 4638 1062 1385 8024 3300 3926 676 8024 5836 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 4\n",
      "INFO:tensorflow:tokens: [CLS] 大 陸 連 接 器 成 為 全 球 第 二 大 生 產 國 [SEP] 大 陸 憑 藉 著 生 產 成 本 低 廉 的 優 勢 ， 以 及 外 資 大 舉 登 陸 投 資 設 廠 的 帶 動 ， 使 其 連 接 器 產 值 呈 穩 定 成 長 。 工 研 院 材 料 所 預 估 ， 大 陸 在 全 球 連 接 器 市 場 佔 有 率 今 年 將 達 21 % ， 並 超 越 日 本 ， 躍 升 為 全 球 第 2 大 國 。 根 據 統 計 ， 1998 年 全 球 連 接 器 市 場 規 模 為 336 億 美 元 [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1920 7380 6865 2970 1690 2768 4158 1059 4413 5018 753 1920 4495 4496 1751 102 1920 7380 2731 5964 5865 4495 4496 2768 3315 856 2442 4638 1032 1248 8024 809 1350 1912 6536 1920 5647 4633 7380 2832 6536 6257 2449 4638 2380 1240 8024 886 1071 6865 2970 1690 4496 966 1439 4952 2137 2768 7269 511 2339 4777 7368 3332 3160 2792 7521 844 8024 1920 7380 1762 1059 4413 6865 2970 1690 2356 1842 861 3300 4372 791 2399 2200 6888 8128 110 8024 699 6631 6632 3189 3315 8024 6713 1285 4158 1059 4413 5018 123 1920 1751 511 3418 3087 5186 6243 8024 8368 2399 1059 4413 6865 2970 1690 2356 1842 6211 3563 4158 12073 1023 5401 1039 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    }
   ],
   "source": [
    "from Bert_preparation_function import  chinese_tokenizer\n",
    "\n",
    "tokenizer = chinese_tokenizer()\n",
    "all_news_path = '../../data_processed/news/all_news.p'\n",
    "seq_length = 128\n",
    "news_dict = loading_pickle_file(all_news_path)\n",
    "# Bertinputdata_1 = Inputdata_for_FE(news_dict,text_type = 'content',method = 'single_sentence').Bert_Inputdata()\n",
    "# Bertinputdata_1 = Inputdata_for_FE(news_dict,text_type = 'title',method = 'single_sentence').Bert_Inputdata()\n",
    "Bertinputdata_1 = Inputdata_for_FE(news_dict,text_type = 'title'+'content',method = 'pair_sentence').Bert_Inputdata()\n",
    "features = convert_examples_to_features(\n",
    "                 examples=Bertinputdata_1, seq_length=seq_length, tokenizer=tokenizer)\n",
    "\n",
    "unique_id_to_feature = {}\n",
    "for feature in features:\n",
    "    unique_id_to_feature[feature.unique_id] = feature\n",
    "    input_fn = input_fn_builder(\n",
    "                 features=features, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(bert_config, init_checkpoint, layer_indexes, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    unique_ids = features[\"unique_ids\"]\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    input_type_ids = features[\"input_type_ids\"]\n",
    "\n",
    "    model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=False,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=input_type_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "      raise ValueError(\"Only PREDICT modes are supported: %s\" % (mode))\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    scaffold_fn = None\n",
    "    (assignment_map,\n",
    "     initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(\n",
    "         tvars, init_checkpoint)\n",
    "#     TPU\n",
    "    if use_tpu:\n",
    "\n",
    "      def tpu_scaffold():\n",
    "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "        return tf.train.Scaffold()\n",
    "\n",
    "      scaffold_fn = tpu_scaffold\n",
    "    else:\n",
    "      tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "    tf.logging.info(\"**** Trainable Variables ****\")\n",
    "    for var in tvars:\n",
    "      init_string = \"\"\n",
    "      if var.name in initialized_variable_names:\n",
    "        init_string = \", *INIT_FROM_CKPT*\"\n",
    "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "                      init_string)\n",
    "\n",
    "    all_layers = model.get_all_encoder_layers()\n",
    "\n",
    "    predictions = {\n",
    "        \"unique_id\": unique_ids,\n",
    "    }\n",
    "\n",
    "    for (i, layer_index) in enumerate(layer_indexes):\n",
    "      predictions[\"layer_output_%d\" % i] = all_layers[layer_index]\n",
    "\n",
    "    output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "        mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f7adddad9d8>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpxqg3p39v\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpxqg3p39v', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7ab13209e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "layer_indexes = [int(x) for x in layers[0].split(\",\")]\n",
    "bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "       vocab_file= vocab_file, do_lower_case=True)\n",
    "#### tpu ####\n",
    "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "          num_shards=num_tpu_cores,\n",
    "          per_host_input_for_training=is_per_host))\n",
    "\n",
    "#### tpu ####\n",
    "model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=init_checkpoint,\n",
    "      layer_indexes=layer_indexes,\n",
    "      use_tpu=False,use_one_hot_embeddings=False)\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "      use_tpu=False,\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      predict_batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Could not find trained model in model_dir: /tmp/tmpxqg3p39v, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running infer on CPU\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (21128, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "45000\n",
      "45500\n",
      "46000\n",
      "46500\n",
      "47000\n",
      "47500\n",
      "48000\n",
      "48500\n",
      "文章數 48503\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "with codecs.getwriter(\"utf-8\")(tf.gfile.Open('output_title_content.jsonl',\"w\")) as writer:\n",
    "    text_counter = 0                                                           #文章數量\n",
    "    for result in estimator.predict(input_fn, yield_single_examples=True):     #每篇文章\n",
    "#         print('article',result['unique_id'], news_dict[result['unique_id']]['new_id'])\n",
    "        text_counter +=1                                                       #文章數量計算\n",
    "        doc_vector = []                                                        #文章向量\n",
    "        unique_id = int(result[\"unique_id\"])\n",
    "        feature = unique_id_to_feature[unique_id]\n",
    "        output_json = collections.OrderedDict()\n",
    "        output_json[\"linex_index\"] = unique_id\n",
    "        all_features = []\n",
    "########## chia-hui modified ##########\n",
    "        word_counter=0\n",
    "        all_feature_extract = np.zeros([4,768])\n",
    "        single_token_layer = []\n",
    "        if unique_id%500 ==0:                                                 # Print 當前進度\n",
    "            print(unique_id)\n",
    "########## chia-hui modified ##########\n",
    "        for (i, token) in enumerate(feature.tokens):                           #文章中每一個字\n",
    "            all_layers = []\n",
    "            word_counter+=1\n",
    "            for (j, layer_index) in enumerate(layer_indexes):                  #前四層\n",
    "                layer_output = result[\"layer_output_%d\" % j]\n",
    "                layers = collections.OrderedDict()\n",
    "                layers[\"index\"] = layer_index\n",
    "                layers[\"values\"] = [\n",
    "                              round(float(x), 6) for x in layer_output[i:(i + 1)].flat]\n",
    "                \n",
    "########## chia-hui modified ##########\n",
    "#                 print(word_counter,i,token,j,layer_index)\n",
    "                if j==0:\n",
    "                    single_token_layer.append(layers[\"values\"])\n",
    "                    for unit in range(768):\n",
    "                        all_feature_extract[0][unit] = all_feature_extract[0][unit] + layers[\"values\"][unit]\n",
    "                elif j==1:\n",
    "                    for unit in range(768):\n",
    "                        all_feature_extract[1][unit] = all_feature_extract[1][unit] + layers[\"values\"][unit]\n",
    "                elif j==2:\n",
    "                    for unit in range(768):\n",
    "                        all_feature_extract[2][unit] = all_feature_extract[2][unit] + layers[\"values\"][unit]\n",
    "                else:\n",
    "                    for unit in range(768):\n",
    "                        all_feature_extract[3][unit] = all_feature_extract[3][unit] + layers[\"values\"][unit]\n",
    "########## chia-hui modified ##########\n",
    "\n",
    "#                 all_layers.append(layers)\n",
    "#             features = collections.OrderedDict()\n",
    "#             features[\"token\"] = token                                         #EX: \"token\": \"[CLS]\"\n",
    "#             features[\"layers\"] = all_layers                                   #EX: \"layers\": [{\"index\": -1, \"values\": [1.073043,...]}]\n",
    "#             all_features.append(features)\n",
    "\n",
    "########## chia-hui modified ##########\n",
    "        all_feature_extract = (all_feature_extract/word_counter).round(5)                # 文章向量(4,768), 總和取字數平均\n",
    "#         print('all_feature_extract',all_feature_extract)\n",
    "        all_layer_feature = collections.OrderedDict()\n",
    "        for layer_id in range(len(layer_indexes)):\n",
    "            all_layer_feature[str(layer_indexes[layer_id])] = all_feature_extract[layer_id].tolist()\n",
    "        doc_vector.append(all_layer_feature)\n",
    "########## chia-hui modified ##########\n",
    "#         print('doc_vector',doc_vector)\n",
    "#         output_json[\"features\"] = all_features\n",
    "        output_json[\"new_id\"] = news_dict[result['unique_id']]['new_id']\n",
    "        output_json[\"doc_vector\"] = doc_vector\n",
    "        writer.write(json.dumps(output_json) + \"\\n\")\n",
    "    print('文章數',text_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(,'wb') as file:\n",
    "            pickle.dump(,file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-21-390f0844bff8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-390f0844bff8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    --NotebookApp.iopub_data_rate_limit=2147483647\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "with open(\"output_title.jsonl\",'r') as file:\n",
    "    for line in file.readlines():\n",
    "        dic = json.loads(line)\n",
    "        print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-eab7b8107d95>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-eab7b8107d95>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python extract_features.py   --input_file=/tmp/input.txt   --output_file=/tmp/output.jsonl   --vocab_file=$BERT_BASE_DIR/vocab.txt   --bert_config_file=$BERT_BASE_DIR/bert_config.json   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt   --layers=-1,-2,-3,-4   --max_seq_length=128   --batch_size=8\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python extract_features.py \\\n",
    "  --input_file=/tmp/input.txt \\\n",
    "  --output_file=/tmp/output.jsonl \\\n",
    "  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n",
    "  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n",
    "  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n",
    "  --layers=-1,-2,-3,-4 \\\n",
    "  --max_seq_length=128 \\\n",
    "  --batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48503"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dict[0]['date']\n",
    "len(news_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'date': '1999-02-10',\n",
       "  'new_id': '0b7a5750-7bcd-11d3-98ff-00e018a00403',\n",
       "  'title': '三大電腦物流業者去年倒帳損失超過1億',\n",
       "  'class': ['C023350'],\n",
       "  'create_date': '1999-02-10 00:00:37',\n",
       "  'content': '1998年由於景氣低迷, 連帶倒帳風險也大為提高, 國內三大物流商聯強、捷元、展碁去年倒帳金額, 合計已經逾新台幣1 億元, 由於物流業毛利率在5%以下, 淨利率更低於1%, 風險損失金額逼近此一比例的業者, 將難以立足市場。'},\n",
       " {'date': '1999-06-02',\n",
       "  'new_id': '0b7a5b20-7bcd-11d3-98ff-00e018a00403',\n",
       "  'title': '晶圓代工市場發燒台積電買德碁多一座八吋廠',\n",
       "  'class': ['C023180'],\n",
       "  'create_date': '1999-06-02 10:34:31',\n",
       "  'content': '台積電將出資金額應在新台幣80億元以上，在近期內宣佈購入德碁半導體公司三成股權，德碁也將成為台積電另一個晶圓代工的場所。股權改組後的德碁，原本最大單一法人股東宏電，持股將由目前的49.3%降至30%，而德碁也將於88年第2季進行50億至60億元的現金增資，實收金額暫訂為100億元，年底直接申請第三類股票上市。目前德碁資本額為208億元，最大股東為宏碁集團，包括宏電、明電、宏科等集團事業單位，持股合計近58%，其中宏電持股便高達49.3%。<SEP>台積電預計拿下的股權，大部分是由宏電釋出，另有中華開發等原股東釋股。由於86、87年德碁虧損金額均超過50億元，88年下半年德碁準備辦理為85年發行的特別股部份減資16億元，將資本額降至192億元，再增資50億至60億元，台積電並將參與認購，以溢價15元至20元之間買入，準備將資本額提高至250億元至270億元，在完成下半年減資、增資後股本之後，總計台積電將持有德碁的股數在7億5,000萬股至8億1,000萬股之間，若以每股面值10元計算，台積電出資的金額將達80億元左右。<SEP>由於德碁過去一直在和IBM談引進晶圓代工技術引進問題，可是長久以來都沒見結果之下，於是施振榮轉向與他交情匪淺的張忠謀合作，且施振榮最近已多次對外公開表示，德碁將朝晶圓代工領域發展，逐漸退出ＤＲＡＭ(動態隨機存取記憶體)製造，此次台積電雖拿下德碁三成股權，將把先進的晶圓代工技術與管理模式導入德碁，一方面讓德碁順利轉型為晶圓代工廠，另一方面補足台積電產能不足的部位；雖然台積電與宏電並列為德碁最大的單一法人股東，再加上明電、宏科等仍為德碁股東，整個宏碁集團仍是德碁最大的股東。所以估計，董事長一職仍將由德碁董事長施振榮擔任，這與聯電入主合泰的模式並不相同。'},\n",
       " {'date': '1999-06-09',\n",
       "  'new_id': '0b7a5c45-7bcd-11d3-98ff-00e018a00403',\n",
       "  'title': '味王樹林廠申請變更被退回',\n",
       "  'class': ['C014010'],\n",
       "  'create_date': '1999-06-09 11:38:21',\n",
       "  'content': '味王食品樹林廠申請變更住商混合區案，8日遭台北縣都市計畫委員會正式發文退回，並要求味王重新提出整體開發規劃、補足相關文件後再審。味王台北樹林廠面積共約1萬8,000多坪，從88年初起即陸續資遣員工，並將生產線遷往雲林豐田廠。'},\n",
       " {'date': '1999-06-11',\n",
       "  'new_id': '0b7a5cf7-7bcd-11d3-98ff-00e018a00403',\n",
       "  'title': '多家印刷電路板上市公司相繼接獲國際大廠訂單',\n",
       "  'class': ['C023210'],\n",
       "  'create_date': '1999-06-11 09:12:53',\n",
       "  'content': '印刷電路板多家印刷電路板上市公司，相繼接獲國際大廠訂單，預估6月營業收入將優於5月，營運漸入佳境，88年5月營收比87年同期成長2成以上的印刷電路板上市公司，有金像電，燿華，敬鵬等3家，營收衰退的公司，有清三，華通，楠梓電子等3家，成長1成以上的公司，計有燿華及敬鵬兩家。<SEP>成長1成以上的公司，計有燿華及敬鵬兩家。<SEP>由於華通，金像電，燿華等公司擴建生產線相繼投產，產能增加，一旦景氣從谷底翻升，單月營收將有再創歷史新高機會，華通電腦及楠梓電子所開發的覆晶承載基板新產品，已通過英特爾認證，88年第4季將陸續投產，華通電腦，燿華電子及元豐電子所開發出的Rambus基板相關產品，也將於第3季或第4季投產，未來營運將有成長空間。<SEP>由於華通，金像電，燿華等公司擴建生產線相繼投產，產能增加，一旦景氣從谷底翻升，單月營收將有再創歷史新高機會，華通電腦及楠梓電子所開發的覆晶承載基板新產品，已通過英特爾認證，88年第4季將陸續投產，華通電腦，燿華電子及元豐電子所開發出的Rambus基板相關產品，也將於第3季或第4季投產，未來營運將有成長空間。<SEP>燿華電子最近接獲美國Qualcomm公司大哥大基板訂單，初期每月出貨量為4萬片，未來將逐月增加，6月營收可超過4億元，7月將挑戰4.5億元單月最高營收紀錄；楠梓電子最近也接獲Nokia及美商Qualcomm公司訂單，未來很可能成為國內最大通信板生產廠商。<SEP>燿華電子最近接獲美國Qualcomm公司大哥大基板訂單，初期每月出貨量為4萬片，未來將逐月增加，6月營收可超過4億元，7月將挑戰4.5億元單月最高營收紀錄；<SEP>楠梓電子最近也接獲Nokia及美商Qualcomm公司訂單，未來很可能成為國內最大通信板生產廠商。<SEP>金像電子桃園新廠將於88年10月完工，89年第1季投產，將生產BGA基板及高階印刷電路板等產品；台光電子因銅箔基板價格滑落，前5月營收比87年同期小幅減少，大陸廠已開始投產，88年獲利仍可優於87年，另外已跨入鋰電池領域，朝向多角化經營發展。<SEP>金像電子桃園新廠將於88年10月完工，89年第1季投產，將生產BGA基板及高階印刷電路板等產品；<SEP>台光電子因銅箔基板價格滑落，前5月營收比87年同期小幅減少，大陸廠已開始投產，88年獲利仍可優於87年，另外已跨入鋰電池領域，朝向多角化經營發展。'},\n",
       " {'date': '1999-06-11',\n",
       "  'new_id': '0b7a5d00-7bcd-11d3-98ff-00e018a00403',\n",
       "  'title': '大陸連接器成為全球第二大生產國',\n",
       "  'class': ['C023120'],\n",
       "  'create_date': '1999-06-11 09:28:43',\n",
       "  'content': '大陸憑藉著生產成本低廉的優勢，以及外資大舉登陸投資設廠的帶動，使其連接器產值呈穩定成長。工研院材料所 預估，大陸在全球連接器市場佔有率今年將達21%，並超越日本，躍升為全球第2大國。根據統計，1998年全球連接器市場規模為336億美元，比1997年衰退6.5%，預估1999年市場規模將與98年相仿。其中，美國、日本分列全球前兩大連接器生產國，在全球前十大廠當中，美商佔有6席，日本則有3家廠商上榜，此外，法國廠商FCI//Berg則排名第2。至於台灣大廠鴻海精密，98年產值則排名全球第11，可望在99年擠進前十大。 根據統計，中國大陸1998年連接器產值達60億美元，全球市場佔有率提升為18%，預估99年可進一步挑戰70億美元，市場佔有率則提升至21%。在全球排名當中，則可望超越日本，躍升第二。推動大陸連接器產業的力量當中，外資扮演相當重要的角色。以1998年來講，大陸本土廠商對整體產值總計貢獻了47%，廠商家數眾多但規模小、設備不足為其特點，設廠地點則集中在江蘇及浙江地區。外商方面，則資金充裕、規模大，製程也比較完整，設廠地點以北京、上海、廣東等地居多。其中以歐美廠商赴大陸投資動作最大，出貨金額佔98年整體產值的20%；其次為台商、日商，分別佔佔13%、8%的比率。另一方面，外資在大陸投資設廠的動作也持續擴大。根據日本媒體的問卷調查顯示，全日本在海外設有生產基地的連接器廠商將近七成，且幾乎每一家都在中國大陸設有工廠。而美國連接器廠商未來五年之內，進駐大陸生產基地的員工總數將達4萬5,000人。台灣連接器廠商目前約有半數前往大陸設廠，其中有二成左右的投資廠商設有兩座以上的廠房，而鴻海轉投資的富士康電腦，目前已是中國大陸連接器第一大廠。1998年，台灣連接器總產值為新台幣470億元，但其中在大陸生產的比率，超過一半，達51%。工研院材料所預估，1999年大陸生產比率將進一步提高到55%。工研院材料所 表示，跟其他國家比起來，中國大陸連接器產業，在價格方面佔有優勢，供貨能力則與歐、美、日廠商不相上下，但略遜於台灣。技術、服務、品質則處於劣勢，其未來發展最大的威脅，則在於當地廠商大多停留在組裝加工的階段，延緩了產業升級的速度。'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dict[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
